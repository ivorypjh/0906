{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c54198",
   "metadata": {},
   "source": [
    "## 공용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf038ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬\n",
    "# ≥3.5 필수\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해 %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "# 이미지를 저장할 디렉토리 생성\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "# 이미지 저장\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"그림 저장:\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "import platform\n",
    "\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "    \n",
    "    \n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# Jupyter Notebook의 출력을 소수점 이하 3자리로 제한\n",
    "%precision 3\n",
    "\n",
    "# 그래픽 출력을 좀 더 고급화하기 위한 라이브러리\n",
    "import seaborn as sns\n",
    "\n",
    "# 과학 기술 통계 라이브러리\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "# 사이킷런 ≥0.20 필수\n",
    "# 0.20 이상 버전에서 데이터 변환을 위한 Transformer 클래스가 추가됨\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# 노트북 실행 결과를 동일하게 유지하기 위해 시드 고정\n",
    "# 데이터를 분할할 때 동일한 분할을 만들어 냄\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d078325",
   "metadata": {},
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5609c23",
   "metadata": {},
   "source": [
    "### 뉴스 그룹 분류 ( 이어서 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067ad8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset = 'all', random_state = 21)\n",
    "# 가져온 데이터의 키 확인\n",
    "# sklearn 에서 datasets 서브 패키지를 이용하면 가져온 데이터는 dict 형태\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187c3c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "#print(news_data.filenames)\n",
    "print(news_data.target_names)\n",
    "# target 의 클래스 이름\n",
    "#print(news_data.target)\n",
    "# 데이터에 대한 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f18b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# target 의 분포 확인\n",
    "# target 은 숫자로 되어 있음\n",
    "# 분포가 한 쪽으로 치우치게 되면 데이터를 층화 추출 하거나 오버 샘플링,\n",
    "# 언더 샘플링, 로그 변환 등을 고려해봐야 하므로 분포를 확인해봐야 함\n",
    "# 정규 분포에서 성능이 가장 좋게 나옴\n",
    "print(pd.Series(news_data.target).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ae1b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "\n",
    "# header 나 footers, quoies 를 제거하고 가져오기\n",
    "train_news = fetch_20newsgroups(subset = 'train', random_state = 21,\n",
    "                               remove = ('headers', 'footers', 'quotes'))\n",
    "# 훈련 데이터 생성\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target # 실제 데이터에서 이게 없으면 군집\n",
    "print(type(X_train)) # 문자열의 list\n",
    "print(type(y_train)) # ndarray\n",
    "#print(X_train[:5])\n",
    "\n",
    "# 테스트 데이터\n",
    "test_news = fetch_20newsgroups(subset = 'test', random_state = 21,\n",
    "                               remove = ('headers', 'footers', 'quotes'))\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd47188",
   "metadata": {},
   "source": [
    "### 피처 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4cce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 단어가 등장한 갯수 기반의 벡터화를 위한 인스턴스 생성\n",
    "cnt_vect = CountVectorizer()\n",
    "\n",
    "# 벡터화\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "print(type(X_train_cnt_vect)) # sparse matrix 형태\n",
    "print(type(X_train_cnt_vect[0])) \n",
    "print(type(X_train_cnt_vect[0][0][0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6bac43",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀를 이용한 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdbd3e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.5967870419543282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression(solver = 'lbfgs', max_iter = 1000)\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "\n",
    "print('정확도 :', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d958580",
   "metadata": {},
   "source": [
    "### TF-IDF 를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fe693ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.6736590546999469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 벡터화에 TF-IDF 를 이용\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "# 벡터화 이후에는 동일하게 선형 회귀 적용\n",
    "lr_clf = LogisticRegression(solver = 'lbfgs', max_iter = 1000)\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "\n",
    "print('정확도 :', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7531853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.6922464152947424\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 를 이용할 때 파라미터를 설정\n",
    "\n",
    "# 위와 동일한 TF-IDF 방식에서 인스턴스를 생성할 때 stop words 를 추가\n",
    "tfidf_vect = TfidfVectorizer(stop_words = 'english', \n",
    "                             ngram_range = (1, 2), max_df = 300)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression(solver = 'lbfgs', max_iter = 1000)\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "\n",
    "print('정확도 :', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c94e6",
   "metadata": {},
   "source": [
    "## 감성 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b844a9",
   "metadata": {},
   "source": [
    "### 나이브 베이즈를 이용한 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fefd189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do', 'not', 'like', 'love', 'you', 'hate'}\n"
     ]
    }
   ],
   "source": [
    "# 샘플 데이터\n",
    "train = [('like you', 'pos'), ('do not like you', 'neg'),\n",
    "        ('hate you', 'neg'), ('do not hate you', 'pos'),\n",
    "        ('love you', 'pos'), ('do not love you', 'neg') ]\n",
    "# TF-IDF 방식은 여러 문장에서 자주 등장하는 단어에 페널티를 부여해서\n",
    "# 기여도를 낮추는 방식\n",
    "\n",
    "\n",
    "# 등장한 모든 단어 찾기\n",
    "\n",
    "from nltk.tokenize import word_tokenize # 단어별로 나누기\n",
    "import nltk\n",
    "\n",
    "#단어 단위로 분할해서 등장한 모든 단어 찾기\n",
    "# 2중 for문을 사용하는데 더 큰 단위(sentence)가 앞에 옴\n",
    "allwords = set(word.lower() for sentence in train\n",
    "              for word in word_tokenize(sentence[0]))\n",
    "\n",
    "print(allwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b0e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'do': False, 'not': False, 'like': True, 'love': False, 'you': True, 'hate': False}, 'pos'), ({'do': True, 'not': True, 'like': True, 'love': False, 'you': True, 'hate': False}, 'neg'), ({'do': False, 'not': False, 'like': False, 'love': False, 'you': True, 'hate': True}, 'neg'), ({'do': True, 'not': True, 'like': False, 'love': False, 'you': True, 'hate': True}, 'pos'), ({'do': False, 'not': False, 'like': False, 'love': True, 'you': True, 'hate': False}, 'pos'), ({'do': True, 'not': True, 'like': False, 'love': True, 'you': True, 'hate': False}, 'neg')]\n"
     ]
    }
   ],
   "source": [
    "# 분류기 만들기\n",
    "\n",
    "# 단어 토큰화\n",
    "t = [({word : (word in word_tokenize(x[0])) for word in allwords},\n",
    "    x[1]) for x in train]\n",
    "# 각 문장별로 단어를 포함하고 있는지 여부를 만들고\n",
    "# 그 때의 감성(pos, neg)을 기록\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a72e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      do = False             pos : neg    =      1.7 : 1.0\n",
      "                      do = True              neg : pos    =      1.7 : 1.0\n",
      "                     not = False             pos : neg    =      1.7 : 1.0\n",
      "                     not = True              neg : pos    =      1.7 : 1.0\n",
      "                    hate = False             neg : pos    =      1.0 : 1.0\n",
      "                    hate = True              neg : pos    =      1.0 : 1.0\n",
      "                    like = False             neg : pos    =      1.0 : 1.0\n",
      "                    like = True              neg : pos    =      1.0 : 1.0\n",
      "                    love = False             neg : pos    =      1.0 : 1.0\n",
      "                    love = True              neg : pos    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 분류를 위한 나이브 베이즈 분류기를 이용해서 모델 생성\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()\n",
    "# 각 feature(단어)에 대해 각 감성을 의미하는 비율을 학습해서 보여줌\n",
    "# like, hate 와 같이 1.0:1.0 비율로 나타나는 feature는 불필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce63cff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do': True, 'not': True, 'like': True, 'love': False, 'you': False, 'hate': False}\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "\n",
    "test_sentence = 'do not like flower'\n",
    "test_sentence_features = {word.lower() : (word in word_tokenize(test_sentence.lower()))\n",
    "                         for word in allwords}\n",
    "print(test_sentence_features)\n",
    "print(classifier.classify(test_sentence_features))\n",
    "# 예측 결과는 neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6782d",
   "metadata": {},
   "source": [
    "### 한글 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "963ac0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     나쁘다 = False              긍정 : 부정     =      1.8 : 1.0\n",
      "                      좋다 = False              부정 : 긍정     =      1.8 : 1.0\n",
      "                     날씨가 = False              부정 : 긍정     =      1.7 : 1.0\n",
      "                     날씨가 = True               긍정 : 부정     =      1.4 : 1.0\n",
      "                      너무 = False              긍정 : 부정     =      1.3 : 1.0\n",
      "                      덥다 = False              긍정 : 부정     =      1.3 : 1.0\n",
      "                      맑다 = False              부정 : 긍정     =      1.3 : 1.0\n",
      "                      비가 = False              긍정 : 부정     =      1.3 : 1.0\n",
      "                    시원하다 = False              부정 : 긍정     =      1.3 : 1.0\n",
      "                      온다 = False              긍정 : 부정     =      1.3 : 1.0\n",
      "{'시원하다': False, '매우': False, '나쁘다': False, '맑다': False, '좋다': False, '온다': False, '덥다': True, '날씨가': False, '너무': True, '날이': True, '비가': True}\n",
      "부정\n"
     ]
    }
   ],
   "source": [
    "# 샘플 데이터\n",
    "train = [('날씨가 좋다', '긍정'), ('날씨가 나쁘다', '부정'),\n",
    "     ('날씨가 매우 좋다', '긍정'), ('날씨가 매우 나쁘다', '부정'),\n",
    "     ('날씨가 맑다', '긍정'), ('비가 온다', '부정'),\n",
    "        ('날이 시원하다', '긍정'), ('날이 너무 덥다', '부정')]\n",
    "\n",
    "# 한글만 있다면 lower 함수는 필요 없음\n",
    "allwords = set(word.lower() for sentence in train\n",
    "              for word in word_tokenize(sentence[0]))\n",
    "\n",
    "#print(allwords)\n",
    "\n",
    "# 단어 토큰화\n",
    "t = [({word : (word in word_tokenize(x[0])) for word in allwords},\n",
    "    x[1]) for x in train]\n",
    "#print(t)\n",
    "\n",
    "# 모델 생성\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()\n",
    "\n",
    "# 예측\n",
    "test_sentence = '비가 오는데 날이 너무 덥다'\n",
    "test_sentence_features = {word.lower() : (word in word_tokenize(test_sentence.lower()))\n",
    "                         for word in allwords}\n",
    "print(test_sentence_features)\n",
    "# 조사 등의 문제로 인해 서로 다른 단어로 인식\n",
    "# 형태소 분석이 필요함\n",
    "print(classifier.classify(test_sentence_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8e211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['날씨/Noun', '가/Josa', '좋다/Adjective'], '긍정'), (['날씨/Noun', '가/Josa', '나쁘다/Adjective'], '부정'), (['날씨/Noun', '가/Josa', '매우/Noun', '좋다/Adjective'], '긍정'), (['날씨/Noun', '가/Josa', '매우/Noun', '나쁘다/Adjective'], '부정'), (['날씨/Noun', '가/Josa', '맑다/Adjective'], '긍정'), (['비/Noun', '가/Josa', '오다/Verb'], '부정'), (['날/Noun', '이/Josa', '시원하다/Adjective'], '긍정'), (['날/Noun', '이/Josa', '너무/Adverb', '덥다/Adjective'], '부정')]\n"
     ]
    }
   ],
   "source": [
    "# 한글 형태소 분석\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "# 문장 단위로 형태소 분석기를 적용해서 \n",
    "# 단어와 품사를 / 로 구분하여 추출\n",
    "def tokenizing(doc):\n",
    "    return [\"/\".join(t) for t in twitter.pos(doc, norm = True, stem = True)]\n",
    "\n",
    "train_docs = [(tokenizing(row[0]), row[1]) for row in train]\n",
    "print(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85545196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날씨/Noun', '가/Josa', '좋다/Adjective', '날씨/Noun', '가/Josa', '나쁘다/Adjective', '날씨/Noun', '가/Josa', '매우/Noun', '좋다/Adjective', '날씨/Noun', '가/Josa', '매우/Noun', '나쁘다/Adjective', '날씨/Noun', '가/Josa', '맑다/Adjective', '비/Noun', '가/Josa', '오다/Verb', '날/Noun', '이/Josa', '시원하다/Adjective', '날/Noun', '이/Josa', '너무/Adverb', '덥다/Adjective']\n"
     ]
    }
   ],
   "source": [
    "# 감성을 빼고 단어만 추출하기\n",
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e917234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'날씨/Noun': True, '가/Josa': True, '좋다/Adjective': True, '나쁘다/Adjective': False, '매우/Noun': False, '맑다/Adjective': False, '비/Noun': False, '오다/Verb': False, '날/Noun': False, '이/Josa': False, '시원하다/Adjective': False, '너무/Adverb': False, '덥다/Adjective': False}, '긍정'), ({'날씨/Noun': True, '가/Josa': True, '좋다/Adjective': False, '나쁘다/Adjective': True, '매우/Noun': False, '맑다/Adjective': False, '비/Noun': False, '오다/Verb': False, '날/Noun': False, '이/Josa': False, '시원하다/Adjective': False, '너무/Adverb': False, '덥다/Adjective': False}, '부정'), ({'날씨/Noun': True, '가/Josa': True, '좋다/Adjective': True, '나쁘다/Adjective': False, '매우/Noun': True, '맑다/Adjective': False, '비/Noun': False, '오다/Verb': False, '날/Noun': False, '이/Josa': False, '시원하다/Adjective': False, '너무/Adverb': False, '덥다/Adjective': False}, '긍정'), ({'날씨/Noun': True, '가/Josa': True, '좋다/Adjective': False, '나쁘다/Adjective': True, '매우/Noun': True, '맑다/Adjective': False, '비/Noun': False, '오다/Verb': False, '날/Noun': False, '이/Josa': False, '시원하다/Adjective': False, '너무/Adverb': False, '덥다/Adjective': False}, '부정'), ({'날씨/Noun': True, '가/Josa': True, '좋다/Adjective': False, '나쁘다/Adjective': False, '매우/Noun': False, '맑다/Adjective': True, '비/Noun': False, '오다/Verb': False, '날/Noun': False, '이/Josa': False, '시원하다/Adjective': False, '너무/Adverb': False, '덥다/Adjective': False}, '긍정'), ({'날씨/Noun': False, '가/Josa': True, '좋다/Adjective': False, '나쁘다/Adjective': False, '매우/Noun': False, '맑다/Adjective': False, '비/Noun': True, '오다/Verb': True, '날/Noun': False, '이/Josa': False, '시원하다/Adjective': False, '너무/Adverb': False, '덥다/Adjective': False}, '부정'), ({'날씨/Noun': False, '가/Josa': False, '좋다/Adjective': False, '나쁘다/Adjective': False, '매우/Noun': False, '맑다/Adjective': False, '비/Noun': False, '오다/Verb': False, '날/Noun': True, '이/Josa': True, '시원하다/Adjective': True, '너무/Adverb': False, '덥다/Adjective': False}, '긍정'), ({'날씨/Noun': False, '가/Josa': False, '좋다/Adjective': False, '나쁘다/Adjective': False, '매우/Noun': False, '맑다/Adjective': False, '비/Noun': False, '오다/Verb': False, '날/Noun': True, '이/Josa': True, '시원하다/Adjective': False, '너무/Adverb': True, '덥다/Adjective': True}, '부정')]\n"
     ]
    }
   ],
   "source": [
    "# 분류기 만들기 - 각 문장에 단어의 존재 여부를 확인해주는 함수\n",
    "# 원리는 영어와 동일하나 한글이라 조금 더 복잡함\n",
    "def term_exists(doc):\n",
    "    return {word : (word in set(doc)) for word in tokens}\n",
    "\n",
    "# 모든 문장을 해석해서 단어의 존재 여부와 감성을 가진 튜플의 리스트를 생성\n",
    "train_xy = [(term_exists(d), c) for d, c in train_docs]\n",
    "print(train_xy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c847ff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           나쁘다/Adjective = False              긍정 : 부정     =      1.8 : 1.0\n",
      "            좋다/Adjective = False              부정 : 긍정     =      1.8 : 1.0\n",
      "                 날씨/Noun = False              부정 : 긍정     =      1.7 : 1.0\n",
      "                 날씨/Noun = True               긍정 : 부정     =      1.4 : 1.0\n",
      "               너무/Adverb = False              긍정 : 부정     =      1.3 : 1.0\n",
      "            덥다/Adjective = False              긍정 : 부정     =      1.3 : 1.0\n",
      "            맑다/Adjective = False              부정 : 긍정     =      1.3 : 1.0\n",
      "                  비/Noun = False              긍정 : 부정     =      1.3 : 1.0\n",
      "          시원하다/Adjective = False              부정 : 긍정     =      1.3 : 1.0\n",
      "                 오다/Verb = False              긍정 : 부정     =      1.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_xy)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e9e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정\n"
     ]
    }
   ],
   "source": [
    "test_sentence = [('날씨를 보니 곧 비가 온 뒤에 더워질 예정이래')]\n",
    "test_docs = twitter.pos(test_sentence[0])\n",
    "test_sentence_features = {word : (word in tokens) for word in test_docs}\n",
    "print(classifier.classify(test_sentence_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cf957",
   "metadata": {},
   "source": [
    "### IMDB 데이터를 이용한 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47cafff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  sentiment                                             review\n",
      "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
      "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
      "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
      "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
      "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "# tsv 파일이므로 seperator 는 탭\n",
    "review_df = pd.read_csv('./data/kaggle_movie/labeledTrainData.tsv', header = 0,\n",
    "                       sep = '\\t', quoting = 3)\n",
    "# id 는 review 를 구분하기 위한 데이터\n",
    "# sentiment 는 감성인데 1이면 긍정이고 2면 부정\n",
    "# review 는 리뷰 데이터\n",
    "print(review_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4faa9c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     With all this stuff going down at the moment ...\n",
      "1       The Classic War of the Worlds   by Timothy ...\n",
      "2     The film starts with a manager  Nicholas Bell...\n",
      "3     It must be assumed that those who praised thi...\n",
      "4     Superbly trashy and wondrously unpretentious ...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 정규식을 이용해서 불필요한 데이터 제거\n",
    "import re\n",
    "\n",
    "# 줄바꿈을 공백으로 제거\n",
    "review_df['review'] = review_df['review'].str.replace('<br/ >', ' ')\n",
    "# 영어가 아닌 것들은 공백으로 그 이외의 것들은 그대로 사용\n",
    "review_df['review'] = review_df['review'].apply(lambda x : re.sub('[^a-zA-Z]', ' ', x))\n",
    "\n",
    "# 확인 - 따옴표, \\ 등 불필요한 내용이 제거됨\n",
    "print(review_df['review'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11ec72cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    1\n",
      "Name: sentiment, dtype: int64\n",
      "                                              review\n",
      "0   With all this stuff going down at the moment ...\n",
      "1     The Classic War of the Worlds   by Timothy ...\n",
      "2   The film starts with a manager  Nicholas Bell...\n",
      "3   It must be assumed that those who praised thi...\n",
      "4   Superbly trashy and wondrously unpretentious ...\n",
      "(17500, 1) (7500, 1)\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 테스트 데이터를 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 타겟\n",
    "class_df = review_df['sentiment']\n",
    "# 피처\n",
    "feature_df = review_df.drop(['id', 'sentiment'], axis = 1, inplace = False)\n",
    "\n",
    "print(class_df.head())\n",
    "print(feature_df.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, class_df,\n",
    "                                                   test_size = 0.3, random_state = 21)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cb4474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.8804\n",
      "ROC_AUC : 0.9484153149245135\n"
     ]
    }
   ],
   "source": [
    "# 훈련 및 예측\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 파이프라인 생성\n",
    "# CountVectorizer 를 사용\n",
    "# TF-IDF 를 사용할 수도 있음\n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', CountVectorizer(stop_words = 'english', ngram_range = (1, 2))),\n",
    "    ('lr_clf', LogisticRegression(C = 10))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:, 1]\n",
    "\n",
    "print('정확도 :', accuracy_score(y_test, pred))\n",
    "print('ROC_AUC :', roc_auc_score(y_test, pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af905b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.8893333333333333\n",
      "ROC_AUC : 0.958380967280847\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 생성 - TF-IDF 적용\n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))),\n",
    "    ('lr_clf', LogisticRegression(C = 10))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:, 1]\n",
    "\n",
    "print('정확도 :', accuracy_score(y_test, pred))\n",
    "print('ROC_AUC :', roc_auc_score(y_test, pred_probs))\n",
    "# 영문의 경우 TF-IDF 를 적용했을 때 좀 더 나은 결과를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d446f4c",
   "metadata": {},
   "source": [
    "## 네이버 식당 리뷰 데이터를 이용한 한글 지도 학습 기반의 감성 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d942b48",
   "metadata": {},
   "source": [
    "### 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bca291ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   score                      review  y\n",
      "0      5            친절하시고 깔끔하고 좋았습니다  1\n",
      "1      5                  조용하고 고기도 굿  1\n",
      "2      4      갈비탕과 냉면, 육회비빔밥이 맛있습니다.  1\n",
      "3      4  대체적으로 만족하나\\n와인의 구성이 살짝 아쉬움  1\n",
      "4      5       고기도 맛있고 서비스는 더 최고입니다~  1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/review_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec57157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   score  y                   ko_text\n",
      "0      5  1          친절하시고 깔끔하고 좋았습니다\n",
      "1      5  1                조용하고 고기도 굿\n",
      "2      4  1      갈비탕과 냉면 육회비빔밥이 맛있습니다\n",
      "3      4  1  대체적으로 만족하나와인의 구성이 살짝 아쉬움\n",
      "4      5  1      고기도 맛있고 서비스는 더 최고입니다\n"
     ]
    }
   ],
   "source": [
    "# 한글 데이터만 추출하는 함수\n",
    "def korean_text(text):\n",
    "    # ㄱ-ㅣ 는 ㅋ 과 같은 초성을 추출\n",
    "    # 일반 한글은 가-힣 으로 추출\n",
    "    hangle = re.compile('[^ ㄱ-ㅣ 가-힣]')\n",
    "    result = hangle.sub('',text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 온점, 물결표 등을 삭제하고 한글만 남김\n",
    "df['ko_text'] = df['review'].apply(lambda x : korean_text(x))\n",
    "del df['review']\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378dc928",
   "metadata": {},
   "source": [
    "### 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cc5e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['조용하고/Adjective', '고기/Noun', '도/Josa', '굿/Noun']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 단어와 품사의 조합으로 변환\n",
    "def get_pos(x):\n",
    "    tagger = Okt()\n",
    "    pos = tagger.pos(x)\n",
    "    pos = ['{}/{}'.format(word, tag) for word, tag in pos]\n",
    "    \n",
    "    return pos\n",
    "\n",
    "# 하나의 데이터로 확인하기\n",
    "result = get_pos(df['ko_text'][1])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb63712",
   "metadata": {},
   "source": [
    "### 피처 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06e66897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(545, 3030)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 방식을 사용해서 벡터화\n",
    "\n",
    "index_vectorizer = TfidfVectorizer(tokenizer = lambda x : get_pos(x))\n",
    "X = index_vectorizer.fit_transform(df['ko_text'].tolist())\n",
    "print(X.shape)\n",
    "print(type(X)) # sparse matrix 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e658b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'친절하시고/Adjective': 2647, '깔끔하고/Adjective': 428, '좋았습니다/Adjective': 2403, '조용하고/Adjective': 2356, '고\n",
      "친절하시고 깔끔하고 좋았습니다\n",
      "  (0, 2403)\t0.48955631270748484\n",
      "  (0, 428)\t0.6726462183300624\n",
      "  (0, 2647)\t0.5548708693511647\n"
     ]
    }
   ],
   "source": [
    "# 피처 확인\n",
    "print(str(index_vectorizer.vocabulary_)[:100])\n",
    "print(df['ko_text'][0])\n",
    "print(X[0])\n",
    "# sparse matrix 형태로 저장\n",
    "# TF-IDF 를 적용했기 때문에 sparse 와 가중치가 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bcca1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(381, 3030)\n",
      "(164, 3030)\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터와 훈련 데이터 생성\n",
    "\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                            test_size = 0.3, random_state = 21)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b65d381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.9024390243902439\n",
      "정밀도 : 0.9024390243902439\n",
      "재현도 : 1.0\n",
      "f1 score : 0.9487179487179488\n",
      "ROC_AUC : 0.9303209459459459\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련 및 평가 - 선형 회귀 사용\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "lr = LogisticRegression(random_state = 21)\n",
    "lr.fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "pred_probs = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('정확도 :', accuracy_score(y_test, pred))\n",
    "print('정밀도 :', precision_score(y_test, pred))\n",
    "print('재현도 :', recall_score(y_test, pred))\n",
    "print('f1 score :', f1_score(y_test, pred))\n",
    "print('ROC_AUC :', roc_auc_score(y_test, pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "109075a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  16]\n",
      " [  0 148]]\n"
     ]
    }
   ],
   "source": [
    "# 오차 행렬 확인\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, pred)\n",
    "print(conf_matrix)\n",
    "# 모든 결과를 1로 예측한걸 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f65ccfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "0     53\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟의 분포 확인\n",
    "\n",
    "df['y'].value_counts()\n",
    "# 1을 나타내는 데이터가 0의 10배"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a81ec6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.44\n",
      "정밀도 : 1.0\n",
      "재현도 : 0.17647058823529413\n",
      "f1 score : 0.3\n",
      "ROC_AUC : 0.9044117647058824\n"
     ]
    }
   ],
   "source": [
    "# Undersampling 적용\n",
    "\n",
    "# 데이터를 50개만 임의로 선정해서 추출\n",
    "positive_random_idx = df[df['y'] == 1].sample(50, random_state = 21).index.tolist()\n",
    "negative_random_idx = df[df['y'] == 0].sample(50, random_state = 21).index.tolist()\n",
    "\n",
    "# 임의로 추출한 인덱스의 데이터만 가져옴\n",
    "random_idx = positive_random_idx + negative_random_idx\n",
    "sample_X = X[random_idx, :]\n",
    "y = df['y'][random_idx]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_X, y,\n",
    "                                            test_size = 0.25, random_state = 21)\n",
    "\n",
    "# 모델에 적용\n",
    "lr = LogisticRegression(random_state = 21)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_probs = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('정확도 :', accuracy_score(y_test, y_pred))\n",
    "print('정밀도 :', precision_score(y_test, y_pred))\n",
    "print('재현도 :', recall_score(y_test, y_pred))\n",
    "print('f1 score :', f1_score(y_test, y_pred))\n",
    "print('ROC_AUC :', roc_auc_score(y_test, y_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5393d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  0]\n",
      " [14  3]]\n"
     ]
    }
   ],
   "source": [
    "# 오차 행렬 확인\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "# 이전과 다르게 0으로 예측하는 경우와 1로 예측하는 경우 모두 생김\n",
    "# 데이터가 부족해서 정확도는 낮게 나오는듯 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b550834",
   "metadata": {},
   "source": [
    "## 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08424065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가져오기\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# 차원 축소에 관련된 일을 하는 decomposition 패키지\n",
    "\n",
    "# 데이터를 가져올 카테고리를 설정\n",
    "categories = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
    "'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med' ]\n",
    "# sci 는 과학, soc 는 사회, rec 는 레크레이션 등\n",
    "\n",
    "# 카테고리에 해당하는 데이터만 가져오기\n",
    "news_data = fetch_20newsgroups(subset = 'all', random_state = 21,\n",
    "                              remove = ('headers', 'footers', 'quotes'),\n",
    "                              categories = categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c33e441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer Shape : (7862, 1000)\n"
     ]
    }
   ],
   "source": [
    "# vectorize\n",
    "count_vect = CountVectorizer(max_df = 0.95, max_features = 1000, min_df = 2,\n",
    "                            stop_words = 'english', ngram_range = (1, 2))\n",
    "\n",
    "feat_vect = count_vect.fit_transform(news_data.data)\n",
    "print('Count Vectorizer Shape :', feat_vect.shape)\n",
    "#  (7862, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1087e1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    }
   ],
   "source": [
    "# 토픽 모델링\n",
    "\n",
    "# 8개로 숫자를 지정해서 토픽을 나눔 - 군집과 유사\n",
    "lda = LatentDirichletAllocation(n_components = 8, random_state = 21)\n",
    "lda.fit(feat_vect)\n",
    "print(lda.components_.shape)\n",
    "# (8, 1000)\n",
    "# 7862였던 데이터를 8개 토픽으로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb4002e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "don just like know think time people said year good didn say did going ll\n",
      "Topic #2\n",
      "10 medical health 1993 12 disease research cancer patients 20 92 11 april information number\n",
      "Topic #3\n",
      "god people jesus church believe think christ does say christian christians know bible don faith\n",
      "Topic #4\n",
      "file image jpeg program color gif output files format images entry bit use display 24\n",
      "Topic #5\n",
      "like use know does just thanks don problem used ve help need good work want\n",
      "Topic #6\n",
      "israel jews jewish israeli people arab state edu arabs ed war world right peace country\n",
      "Topic #7\n",
      "edu graphics available window com server ftp windows software dos mail version data sun motif\n",
      "Topic #8\n",
      "armenian armenians turkish people dos dos turkey armenia 000 genocide government turks russian azerbaijan greek said\n"
     ]
    }
   ],
   "source": [
    "# 각 토픽에서 중요 N개 단어 추출하는 함수\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #', topic_index + 1, sep = '')\n",
    "    \n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        # 지정한 갯수에 해당하는 단어들의 인덱스를 가져옴\n",
    "        top_indexes = topic_word_indexes[:num_top_words]\n",
    "\n",
    "        # 인덱스를 사용해서 지정한 갯수의 단어를 조합\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n",
    "\n",
    "        print(feature_concat)\n",
    "\n",
    "# 피처 이름 가져오기\n",
    "# sklearn 의 최신 버전에서 get_feature_names 함수가 사라지고\n",
    "# get_feature_names_out 함수로 바뀜\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "\n",
    "\n",
    "display_topics(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbd3e5",
   "metadata": {},
   "source": [
    "## 텍스트 군집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd7b0bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\data\\\\OpinosisDataset\\\\topics\\\\accuracy_garmin_nuvi_255W_gps.txt.data', '.\\\\data\\\\OpinosisDataset\\\\topics\\\\bathroom_bestwestern_hotel_sfo.txt.data', '.\\\\data\\\\OpinosisDataset\\\\topics\\\\battery-life_amazon_kindle.txt.data', '.\\\\data\\\\OpinosisDataset\\\\topics\\\\battery-life_ipod_nano_8gb.txt.data', '.\\\\data\\\\OpinosisDataset\\\\topics\\\\battery-life_netbook_1005ha.txt.data']\n"
     ]
    }
   ],
   "source": [
    "# data/OpinosisDataset/topics 디렉토리에서 데이터 파일 전부 읽기\n",
    "\n",
    "import glob, os\n",
    "import platform\n",
    "\n",
    "# 디렉토리의 이름(경로)을 생성\n",
    "# MAC 은 경로에 / 를 사용하고 Windows 는 \\\\(\\) 를 사용\n",
    "# 파이썬은 제어문이 블럭이 아니라서 제어문 안에서 변수를 생성해도 되지만\n",
    "# 자바와 같은 다른 언어는 제어문이 블럭이므로 변수 이름을 미리 만들고 사용해야 함\n",
    "path_name = \"\"\n",
    "if platform.system() == 'Darwin':\n",
    "    path_name = './data/OpinosisDataset/topics'\n",
    "elif platform.system() == 'Windows':\n",
    "    path_name = '.\\\\data\\\\OpinosisDataset\\\\topics'\n",
    "\n",
    "# 디렉토리 안의 모든 파일 이름을 list 로 생성\n",
    "# path_name 내의'.data' 로 끝나는 모든 파일 이름(파일의 경로)을 가져옴\n",
    "# ~로 시작하는, ~를 중간에 포함하는 등의 방식도 지정 가능\n",
    "all_file_name = glob.glob(os.path.join(path_name, \"*.data\"))\n",
    "print(all_file_name[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e726d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일의 이름을 저장할 list \n",
    "file_name_list = []\n",
    "# 파일의 내용을 저장할 list\n",
    "file_in = []\n",
    "\n",
    "# 파일의 경로를 순회하면서 파일의 내용을 읽어서 하나로 만들기\n",
    "\n",
    "# 파일의 내용 읽기\n",
    "for file in all_file_name:\n",
    "    df = pd.read_table(file_name, index_col = None, header = 0, encoding = 'latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e6c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb091762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351c844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b3c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c492a62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
